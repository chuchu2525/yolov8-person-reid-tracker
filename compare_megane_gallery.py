import cv2
import numpy as np
from ultralytics import YOLO
import torch
import torch.nn.functional as F
import os
from pathlib import Path

# boxmotã®ReIDãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
try:
    from boxmot.appearance.reid.factory import osnet_x0_25
    print("âœ… boxmot imported successfully")
    BOXMOT_AVAILABLE = True
except ImportError:
    print("âš ï¸ boxmot not available, using fallback")
    BOXMOT_AVAILABLE = False

# è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
CONFIG = {
    'min_confidence': 0.5,          # æœ€å°ä¿¡é ¼åº¦é–¾å€¤ï¼ˆæ„Ÿåº¦å‘ä¸Šï¼‰
    'min_area_ratio': 0.005,        # ç”»åƒå…¨ä½“ã«å¯¾ã™ã‚‹æœ€å°é¢ç©æ¯”ï¼ˆã‚ˆã‚Šå°ã•ãªäººç‰©ã‚‚æ¤œå‡ºï¼‰
    'min_width': 40,                # æœ€å°å¹…ï¼ˆç·©å’Œï¼‰
    'min_height': 80,               # æœ€å°é«˜ã•ï¼ˆç·©å’Œï¼‰
    'cosine_threshold': 0.85,       # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦é–¾å€¤
    'nms_iou_threshold': 0.4,       # NMS IoUé–¾å€¤
    'reid_model': 'osnet_x0_25',    # OSNetãƒ¢ãƒ‡ãƒ«
}

class OSNetFeatureExtractor:
    """OSNetãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡æŠ½å‡ºå™¨ï¼ˆmeganeå°‚ç”¨ï¼‰"""
    
    def __init__(self, model_name='osnet_x0_25'):
        self.model = None
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        if BOXMOT_AVAILABLE:
            try:
                print(f"ğŸ§  OSNet({model_name})ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...")
                
                if model_name == 'osnet_x0_25':
                    self.model = osnet_x0_25(num_classes=1000, pretrained=True)
                else:
                    from boxmot.appearance.reid.factory import MODEL_FACTORY
                    if model_name in MODEL_FACTORY:
                        model_func = MODEL_FACTORY[model_name]
                        self.model = model_func(num_classes=1000, pretrained=True)
                    else:
                        raise ValueError(f"Unknown model: {model_name}")
                
                self.model.eval()
                self.model.to(self.device)
                print(f"âœ… OSNetãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº† (device: {self.device})")
                
            except Exception as e:
                print(f"âŒ OSNetãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}")
                print("ğŸ“ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰¹å¾´é‡æŠ½å‡ºå™¨ã‚’ä½¿ç”¨ã—ã¾ã™")
                self.model = None
        else:
            print("ğŸ“ boxmotåˆ©ç”¨ä¸å¯ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰¹å¾´é‡æŠ½å‡ºå™¨ã‚’ä½¿ç”¨")
    
    def extract_features(self, image, box):
        """OSNetã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰¹å¾´é‡æŠ½å‡º"""
        x1, y1, x2, y2 = map(int, box)
        h, w = image.shape[:2]
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(w, x2), min(h, y2)
        
        person_patch = image[y1:y2, x1:x2]
        if person_patch.size == 0 or person_patch.shape[0] < 20 or person_patch.shape[1] < 10:
            return None
        
        if self.model is not None and BOXMOT_AVAILABLE:
            return self._extract_osnet_features(person_patch)
        else:
            return self._extract_fallback_features(person_patch)
    
    def _extract_osnet_features(self, person_patch):
        """OSNetã‚’ä½¿ã£ãŸç‰¹å¾´é‡æŠ½å‡º"""
        try:
            resized = cv2.resize(person_patch, (128, 256))
            rgb_image = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
            
            mean = np.array([0.485, 0.456, 0.406])
            std = np.array([0.229, 0.224, 0.225])
            normalized = (rgb_image / 255.0 - mean) / std
            
            tensor = torch.from_numpy(normalized.transpose(2, 0, 1)).float()
            tensor = tensor.unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                features = self.model(tensor)
                features = F.normalize(features, p=2, dim=1)
                return features.cpu().numpy().flatten()
                
        except Exception as e:
            print(f"âš ï¸ OSNetç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return self._extract_fallback_features(person_patch)
    
    def _extract_fallback_features(self, person_patch):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰¹å¾´é‡æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        resized = cv2.resize(person_patch, (128, 256))
        
        hsv = cv2.cvtColor(resized, cv2.COLOR_BGR2HSV)
        mid_height = 128
        
        upper_hsv = hsv[:mid_height, :]
        lower_hsv = hsv[mid_height:, :]
        
        upper_hist_h = cv2.calcHist([upper_hsv], [0], None, [16], [0, 180])
        upper_hist_s = cv2.calcHist([upper_hsv], [1], None, [16], [0, 256])
        upper_hist_v = cv2.calcHist([upper_hsv], [2], None, [16], [0, 256])
        
        lower_hist_h = cv2.calcHist([lower_hsv], [0], None, [16], [0, 180])
        lower_hist_s = cv2.calcHist([lower_hsv], [1], None, [16], [0, 256])
        lower_hist_v = cv2.calcHist([lower_hsv], [2], None, [16], [0, 256])
        
        gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        edge_hist = cv2.calcHist([edges], [0], None, [16], [0, 256])
        
        features = []
        for hist in [upper_hist_h, upper_hist_s, upper_hist_v, 
                    lower_hist_h, lower_hist_s, lower_hist_v, 
                    edge_hist]:
            normalized = cv2.normalize(hist, hist).flatten()
            features.append(normalized)
        
        combined_features = np.concatenate(features)
        norm = np.linalg.norm(combined_features)
        if norm > 0:
            combined_features = combined_features / norm
        
        return combined_features
    
    def calculate_cosine_similarity(self, feat1, feat2):
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—"""
        if feat1 is None or feat2 is None:
            return None
        
        return np.dot(feat1, feat2) / (np.linalg.norm(feat1) * np.linalg.norm(feat2))

def filter_detections(detections, image_shape, config):
    """æ¤œå‡ºçµæœã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    h, w = image_shape[:2]
    image_area = h * w
    
    filtered = []
    print(f"  ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‰: {len(detections)}å€‹ã®æ¤œå‡º")
    
    for i, detection in enumerate(detections):
        x1, y1, x2, y2 = detection['bbox']
        conf = detection['conf']
        
        if conf < config['min_confidence']:
            print(f"    äººç‰©{i+1}: ä¿¡é ¼åº¦ä¸è¶³ ({conf:.3f} < {config['min_confidence']})")
            continue
        
        width = x2 - x1
        height = y2 - y1
        area = width * height
        area_ratio = area / image_area
        
        if (width < config['min_width'] or 
            height < config['min_height'] or 
            area_ratio < config['min_area_ratio']):
            print(f"    äººç‰©{i+1}: ã‚µã‚¤ã‚ºä¸è¶³ (å¹…{width:.0f}, é«˜{height:.0f}, é¢ç©æ¯”{area_ratio:.4f})")
            continue
        
        filtered.append(detection)
        print(f"    äººç‰©{i+1}: ãƒ•ã‚£ãƒ«ã‚¿é€šé âœ“")
    
    print(f"  ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ: {len(filtered)}å€‹ã®æ¤œå‡º")
    return filtered

def detect_and_extract_features_megane(image_path, model, extractor, config, output_dir="detection_results_megane"):
    """meganeå°‚ç”¨æ¤œå‡ºãƒ»ç‰¹å¾´é‡æŠ½å‡º"""
    print(f"\n=== {image_path} ã®å‡¦ç†ï¼ˆmeganeå°‚ç”¨ï¼‰ ===")
    
    os.makedirs(output_dir, exist_ok=True)
    
    image = cv2.imread(image_path)
    if image is None:
        print(f"ã‚¨ãƒ©ãƒ¼: {image_path} ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ")
        return None, None
    
    print(f"ç”»åƒã‚µã‚¤ã‚º: {image.shape}")
    
    # YOLOv8ã§äººç‰©æ¤œå‡º
    results = model(image, classes=0, conf=config['min_confidence'], 
                   iou=config['nms_iou_threshold'], verbose=False)
    
    raw_detections = []
    
    if results[0].boxes is not None:
        boxes = results[0].boxes
        print(f"YOLOæ¤œå‡ºæ•°: {len(boxes)}")
        
        for i, box in enumerate(boxes):
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            conf = box.conf[0].cpu().numpy()
            
            raw_detections.append({
                'bbox': [x1, y1, x2, y2],
                'conf': conf,
                'index': i
            })
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered_detections = filter_detections(raw_detections, image.shape, config)
    
    # ç‰¹å¾´é‡æŠ½å‡ºã¨å¯è¦–åŒ–
    final_detections = []
    features = []
    
    if filtered_detections:
        vis_image = image.copy()
        
        for i, detection in enumerate(filtered_detections):
            x1, y1, x2, y2 = detection['bbox']
            conf = detection['conf']
            
            # OSNetç‰¹å¾´é‡æŠ½å‡º
            feature = extractor.extract_features(image, [x1, y1, x2, y2])
            
            if feature is not None:
                final_detections.append(detection)
                features.append(feature)
                
                # å¯è¦–åŒ–ï¼ˆã‚ãŒã­å°‚ç”¨ã‚«ãƒ©ãƒ¼ãƒªãƒ³ã‚°ï¼‰
                x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])
                
                # ã‚ãŒã­å°‚ç”¨è‰²ï¼šç´«ã€ã‚ªãƒ¬ãƒ³ã‚¸ã€ç·‘
                colors = [(128, 0, 128), (0, 165, 255), (0, 255, 0), (255, 255, 0), (255, 0, 255), (0, 255, 255)]
                color = colors[i % len(colors)]
                
                cv2.rectangle(vis_image, (x1, y1), (x2, y2), color, 4)
                
                # ãƒ©ãƒ™ãƒ«ï¼ˆã‚ãŒã­å°‚ç”¨ãƒãƒ¼ã‚¯ä»˜ãï¼‰
                model_mark = "ğŸ‘“ğŸ§ " if extractor.model is not None else "ğŸ‘“ğŸ“"
                label = f"Megane Person {i+1} ({conf:.2f}) {model_mark}"
                label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0]
                
                cv2.rectangle(vis_image, (x1, y1-35), (x1+label_size[0]+10, y1), color, -1)
                cv2.putText(vis_image, label, (x1+5, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
                
                # ä¸­å¤®ç•ªå·
                number_text = str(i+1)
                center_x = (x1 + x2) // 2
                center_y = (y1 + y2) // 2
                
                cv2.circle(vis_image, (center_x, center_y), 40, color, -1)
                cv2.circle(vis_image, (center_x, center_y), 40, (255, 255, 255), 4)
                
                text_size = cv2.getTextSize(number_text, cv2.FONT_HERSHEY_SIMPLEX, 1.8, 4)[0]
                text_x = center_x - text_size[0] // 2
                text_y = center_y + text_size[1] // 2
                cv2.putText(vis_image, number_text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1.8, (255, 255, 255), 4)
                
                print(f"  ğŸ‘“ äººç‰©{i+1}: ç‰¹å¾´é‡æŠ½å‡ºæˆåŠŸï¼ˆã‚µã‚¤ã‚º: {len(feature)}ï¼‰")
        
        # ç”»åƒä¿å­˜
        filename = os.path.basename(image_path).split('.')[0]
        output_path = os.path.join(output_dir, f"{filename}_megane_detected.jpg")
        cv2.imwrite(output_path, vis_image)
        print(f"æ¤œå‡ºçµæœã‚’ä¿å­˜: {output_path}")
    
    return final_detections, features

def analyze_megane_similarity(all_results, extractor):
    """meganeç‰¹å¾´é‡é¡ä¼¼åº¦åˆ†æï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""
    print("\n" + "="*80)
    print("ğŸ‘“ Megane Similarity Analysis (megane01~03)")
    print("="*80)

    # 1. meganeç³»åˆ—ã®ç‰¹å¾´é‡åé›†
    print("ğŸ“Š meganeç³»åˆ—ç‰¹å¾´é‡ã®åé›†...")
    
    # å„meganeç”»åƒã‹ã‚‰æœ€åˆã®äººç‰©ã‚’æŠ½å‡ºï¼ˆãƒ¡ã‚¤ãƒ³ã®äººç‰©ã¨ä»®å®šï¼‰
    megane_main_persons = {}
    
    for img_name in ['megane01', 'megane02', 'megane03']:
        if img_name in all_results and len(all_results[img_name]['features']) > 0:
            # æœ€åˆã®äººç‰©ï¼ˆé€šå¸¸ã¯æœ€ã‚‚ä¿¡é ¼åº¦ãŒé«˜ã„ï¼‰ã‚’ãƒ¡ã‚¤ãƒ³äººç‰©ã¨ã™ã‚‹
            megane_main_persons[f"{img_name}_main"] = all_results[img_name]['features'][0]
            print(f"  âœ… {img_name}: ãƒ¡ã‚¤ãƒ³äººç‰©ã®ç‰¹å¾´é‡ã‚’æŠ½å‡º")
    
    if len(megane_main_persons) < 2:
        print("âŒ æ¯”è¼ƒã«å¿…è¦ãªmeganeç‰¹å¾´é‡ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
        return

    # 2. meganeé–“ã®é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ä½œæˆ
    print(f"\nğŸ‘“ meganeé–“ã®é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹:")
    megane_ids = list(megane_main_persons.keys())
    
    print("    ", end="")
    for mid in megane_ids:
        print(f"{mid:>15}", end="")
    print()
    
    high_similarity_pairs = []
    similarity_matrix = []
    
    for i, id1 in enumerate(megane_ids):
        print(f"{id1:>15}", end="")
        row = []
        for j, id2 in enumerate(megane_ids):
            if i == j:
                print(f"{'1.000':>15}", end="")
                row.append(1.0)
            else:
                feat1 = megane_main_persons[id1]
                feat2 = megane_main_persons[id2]
                sim = extractor.calculate_cosine_similarity(feat1, feat2)
                print(f"{sim:>15.3f}", end="")
                row.append(sim)
                
                # é«˜é¡ä¼¼åº¦ãƒšã‚¢ã‚’è¨˜éŒ²
                if sim > 0.7 and i < j:
                    high_similarity_pairs.append((id1, id2, sim))
        similarity_matrix.append(row)
        print()
    
    # 3. åŒä¸€äººç‰©åˆ¤å®šåˆ†æ
    print(f"\nğŸ” åŒä¸€äººç‰©åˆ¤å®šåˆ†æ:")
    
    if high_similarity_pairs:
        print(f"é«˜é¡ä¼¼åº¦ãƒšã‚¢ï¼ˆé–¾å€¤0.7ä»¥ä¸Šï¼‰:")
        high_similarity_pairs.sort(key=lambda x: x[2], reverse=True)
        
        for id1, id2, sim in high_similarity_pairs:
            print(f"\nã€{id1} â†” {id2}ã€‘")
            print(f"  é¡ä¼¼åº¦: {sim:.4f}")
            
            if sim > 0.9:
                print("  ğŸ¯ æ¥µã‚ã¦é«˜ã„é¡ä¼¼åº¦ â†’ åŒä¸€äººç‰©ã®å¯èƒ½æ€§ãŒéå¸¸ã«é«˜ã„")
            elif sim > 0.85:
                print("  ğŸš€ éå¸¸ã«é«˜ã„é¡ä¼¼åº¦ â†’ åŒä¸€äººç‰©ã®å¯èƒ½æ€§ãŒé«˜ã„")
            elif sim > 0.8:
                print("  âœ… é«˜ã„é¡ä¼¼åº¦ â†’ åŒä¸€äººç‰©ã®å¯èƒ½æ€§ã‚ã‚Š")
            else:
                print("  âš ï¸ ä¸­ç¨‹åº¦ã®é¡ä¼¼åº¦ â†’ è¦æ¤œè¨")
    else:
        print("é«˜é¡ä¼¼åº¦ãƒšã‚¢ï¼ˆ0.7ä»¥ä¸Šï¼‰ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        print("â†’ å„meganeç”»åƒã®äººç‰©ã¯ååˆ†ã«åŒºåˆ¥ã•ã‚Œã¦ã„ã¾ã™")
    
    # 4. æœ€å°é¡ä¼¼åº¦åˆ†æ
    min_similarity = 1.0
    min_pair = None
    
    for i, id1 in enumerate(megane_ids):
        for j, id2 in enumerate(megane_ids):
            if i < j:
                sim = similarity_matrix[i][j]
                if sim < min_similarity:
                    min_similarity = sim
                    min_pair = (id1, id2)
    
    print(f"\nğŸ“ˆ åˆ†é›¢æ€§èƒ½è©•ä¾¡:")
    print(f"  æœ€å°é¡ä¼¼åº¦: {min_similarity:.4f} ({min_pair[0]} â†” {min_pair[1]})")
    
    if min_similarity < 0.5:
        print("  ğŸ‰ å„ªç§€ãªåˆ†é›¢æ€§èƒ½ - å„meganeäººç‰©ãŒæ˜ç¢ºã«åŒºåˆ¥ã•ã‚Œã¦ã„ã¾ã™")
    elif min_similarity < 0.7:
        print("  âœ… è‰¯å¥½ãªåˆ†é›¢æ€§èƒ½ - å®Ÿç”¨çš„ãªãƒ¬ãƒ™ãƒ«ã§ã™")
    else:
        print("  âš ï¸ æ³¨æ„ãŒå¿…è¦ - ä¸€éƒ¨meganeäººç‰©ã®åŒºåˆ¥ãŒæ›–æ˜§ã§ã™")
    
    # 5. å…¨äººç‰©çµ±åˆåˆ†æ
    print(f"\nğŸ§‘â€ğŸ¤â€ğŸ§‘ å…¨meganeäººç‰©çµ±åˆåˆ†æ:")
    
    all_megane_features = []
    all_megane_labels = []
    
    for img_name in ['megane01', 'megane02', 'megane03']:
        if img_name in all_results:
            for i, feature in enumerate(all_results[img_name]['features']):
                all_megane_features.append(feature)
                all_megane_labels.append(f"{img_name}_äººç‰©{i+1}")
    
    print(f"ç·äººç‰©æ•°: {len(all_megane_features)}äºº")
    
    if len(all_megane_features) > 1:
        print(f"\nå…¨äººç‰©é–“é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹:")
        print("    ", end="")
        for label in all_megane_labels:
            print(f"{label[:12]:>12}", end="")
        print()
        
        for i, label1 in enumerate(all_megane_labels):
            print(f"{label1[:12]:>12}", end="")
            for j, label2 in enumerate(all_megane_labels):
                if i == j:
                    print(f"{'1.00':>12}", end="")
                else:
                    feat1 = all_megane_features[i]
                    feat2 = all_megane_features[j]
                    sim = extractor.calculate_cosine_similarity(feat1, feat2)
                    print(f"{sim:>12.3f}", end="")
            print()

def main():
    print("ğŸ‘“ megane01~03 ç‰¹å¾´é‡æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹ã—ã¾ã™...")
    print(f"è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {CONFIG}")
    
    # YOLOv8ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    model = YOLO('yolov8m.pt')
    
    # OSNetç‰¹å¾´é‡æŠ½å‡ºå™¨ã‚’åˆæœŸåŒ–
    extractor = OSNetFeatureExtractor(CONFIG['reid_model'])
    
    # meganeç”»åƒãƒ‘ã‚¹
    image_paths = {
        'megane01': "../videos/megane01.png",
        'megane02': "../videos/megane02.png", 
        'megane03': "../videos/megane03.png"
    }
    
    # å…¨çµæœã‚’æ ¼ç´
    all_results = {}
    
    # å„ç”»åƒã‚’å‡¦ç†
    for img_name, img_path in image_paths.items():
        print("=" * 60)
        detections, features = detect_and_extract_features_megane(img_path, model, extractor, CONFIG)
        
        if detections and features:
            all_results[img_name] = {
                'detections': detections,
                'features': features,
                'path': img_path
            }
        else:
            print(f"âš ï¸  {img_name} ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    if len(all_results) < 1:
        print("âŒ æ¯”è¼ƒã«å¿…è¦ãªmeganeç”»åƒãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        return
    
    # meganeé¡ä¼¼åº¦åˆ†æã‚’å®Ÿè¡Œ
    analyze_megane_similarity(all_results, extractor)
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "="*80)
    print("ğŸ“‹ meganeç‰¹å¾´é‡æ¯”è¼ƒçµæœã‚µãƒãƒªãƒ¼")
    print("="*80)
    
    model_info = "ğŸ‘“ğŸ§  OSNet" if extractor.model is not None else "ğŸ‘“ğŸ“ Fallback"
    print(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_info}")
    print(f"å‡¦ç†ç”»åƒæ•°: {len(all_results)}æš")
    
    total_detections = sum(len(result['detections']) for result in all_results.values())
    print(f"ç·æ¤œå‡ºäººæ•°: {total_detections}äºº")
    
    for img_name, result in all_results.items():
        print(f"\n{img_name}: {len(result['detections'])}äººæ¤œå‡ºï¼ˆãƒ•ã‚£ãƒ«ã‚¿å¾Œï¼‰")
        for i, detection in enumerate(result['detections']):
            x1, y1, x2, y2 = detection['bbox']
            conf = detection['conf']
            w, h = x2 - x1, y2 - y1
            print(f"  ğŸ‘“äººç‰©{i+1}: åº§æ¨™({x1:.0f},{y1:.0f}) ã‚µã‚¤ã‚º{w:.0f}x{h:.0f} ä¿¡é ¼åº¦{conf:.3f}")
    
    print(f"\nğŸ–¼ï¸  meganeæ¤œå‡ºçµæœç”»åƒã¯ 'detection_results_megane' ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
    print("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:")
    for img_name in all_results.keys():
        print(f"   - {img_name}_megane_detected.jpg")

if __name__ == "__main__":
    main() 