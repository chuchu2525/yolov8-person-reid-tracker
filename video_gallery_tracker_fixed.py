import cv2
import numpy as np
from ultralytics import YOLO
import torch
import torch.nn.functional as F
import os
import time
from pathlib import Path

# boxmotã®ReIDãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
try:
    from boxmot.appearance.reid.factory import osnet_x0_25
    print("âœ… boxmot imported successfully")
    BOXMOT_AVAILABLE = True
except ImportError:
    print("âš ï¸ boxmot not available, using fallback")
    BOXMOT_AVAILABLE = False

# è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä¿®æ­£ç‰ˆï¼‰
CONFIG = {
    'min_confidence': 0.6,          # æœ€å°ä¿¡é ¼åº¦é–¾å€¤ï¼ˆå°‘ã—ä¸Šã’ã‚‹ï¼‰
    'min_area_ratio': 0.01,         # ç”»åƒå…¨ä½“ã«å¯¾ã™ã‚‹æœ€å°é¢ç©æ¯”ï¼ˆä¸Šã’ã‚‹ï¼‰
    'min_width': 60,                # æœ€å°å¹…ï¼ˆä¸Šã’ã‚‹ï¼‰
    'min_height': 120,              # æœ€å°é«˜ã•ï¼ˆä¸Šã’ã‚‹ï¼‰
    'cosine_threshold': 0.85,       # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦é–¾å€¤
    'nms_iou_threshold': 0.3,       # NMS IoUé–¾å€¤ï¼ˆå³ã—ãã™ã‚‹ï¼‰
    'reid_model': 'osnet_x0_25',    # OSNetãƒ¢ãƒ‡ãƒ«
    'gallery_threshold': 0.7,       # ã‚®ãƒ£ãƒ©ãƒªãƒ¼ãƒãƒƒãƒãƒ³ã‚°é–¾å€¤ï¼ˆå³ã—ãã™ã‚‹ï¼‰
    'max_gallery_size': 20,         # 1äººå½“ãŸã‚Šã®æœ€å¤§ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã‚µã‚¤ã‚ºï¼ˆå‰Šæ¸›ï¼‰
    'update_interval': 3,           # ç‰¹å¾´é‡æ›´æ–°é–“éš”ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼‰
    'max_missing_frames': 9000,     # IDæ¶ˆå¤±åˆ¤å®šãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼ˆ5åˆ†@30FPSæƒ³å®š: 5*60*30=9000ï¼‰
    'video_fps': 10,                # å‡ºåŠ›å‹•ç”»ã®FPS
    'iou_threshold': 0.3,           # æ¤œå‡ºé–“ã®IoUé‡è¤‡é™¤å»é–¾å€¤
}

class OSNetFeatureExtractor:
    """OSNetãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡æŠ½å‡ºå™¨ï¼ˆå‹•ç”»ç”¨ï¼‰"""
    
    def __init__(self, model_name='osnet_x0_25'):
        self.model = None
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        if BOXMOT_AVAILABLE:
            try:
                print(f"ğŸ§  OSNet({model_name})ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...")
                
                if model_name == 'osnet_x0_25':
                    self.model = osnet_x0_25(num_classes=1000, pretrained=True)
                else:
                    from boxmot.appearance.reid.factory import MODEL_FACTORY
                    if model_name in MODEL_FACTORY:
                        model_func = MODEL_FACTORY[model_name]
                        self.model = model_func(num_classes=1000, pretrained=True)
                    else:
                        raise ValueError(f"Unknown model: {model_name}")
                
                self.model.eval()
                self.model.to(self.device)
                print(f"âœ… OSNetãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº† (device: {self.device})")
                
            except Exception as e:
                print(f"âŒ OSNetãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}")
                print("ğŸ“ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰¹å¾´é‡æŠ½å‡ºå™¨ã‚’ä½¿ç”¨ã—ã¾ã™")
                self.model = None
        else:
            print("ğŸ“ boxmotåˆ©ç”¨ä¸å¯ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰¹å¾´é‡æŠ½å‡ºå™¨ã‚’ä½¿ç”¨")
    
    def extract_features(self, image, box):
        """OSNetã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰¹å¾´é‡æŠ½å‡º"""
        x1, y1, x2, y2 = map(int, box)
        h, w = image.shape[:2]
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(w, x2), min(h, y2)
        
        person_patch = image[y1:y2, x1:x2]
        if person_patch.size == 0 or person_patch.shape[0] < 20 or person_patch.shape[1] < 10:
            return None
        
        if self.model is not None and BOXMOT_AVAILABLE:
            return self._extract_osnet_features(person_patch)
        else:
            return self._extract_fallback_features(person_patch)
    
    def _extract_osnet_features(self, person_patch):
        """OSNetã‚’ä½¿ã£ãŸç‰¹å¾´é‡æŠ½å‡º"""
        try:
            resized = cv2.resize(person_patch, (128, 256))
            rgb_image = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
            
            mean = np.array([0.485, 0.456, 0.406])
            std = np.array([0.229, 0.224, 0.225])
            normalized = (rgb_image / 255.0 - mean) / std
            
            tensor = torch.from_numpy(normalized.transpose(2, 0, 1)).float()
            tensor = tensor.unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                features = self.model(tensor)
                features = F.normalize(features, p=2, dim=1)
                return features.cpu().numpy().flatten()
                
        except Exception as e:
            print(f"âš ï¸ OSNetç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return self._extract_fallback_features(person_patch)
    
    def _extract_fallback_features(self, person_patch):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰¹å¾´é‡æŠ½å‡º"""
        resized = cv2.resize(person_patch, (128, 256))
        
        hsv = cv2.cvtColor(resized, cv2.COLOR_BGR2HSV)
        mid_height = 128
        
        upper_hsv = hsv[:mid_height, :]
        lower_hsv = hsv[mid_height:, :]
        
        upper_hist_h = cv2.calcHist([upper_hsv], [0], None, [16], [0, 180])
        upper_hist_s = cv2.calcHist([upper_hsv], [1], None, [16], [0, 256])
        upper_hist_v = cv2.calcHist([upper_hsv], [2], None, [16], [0, 256])
        
        lower_hist_h = cv2.calcHist([lower_hsv], [0], None, [16], [0, 180])
        lower_hist_s = cv2.calcHist([lower_hsv], [1], None, [16], [0, 256])
        lower_hist_v = cv2.calcHist([lower_hsv], [2], None, [16], [0, 256])
        
        gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        edge_hist = cv2.calcHist([edges], [0], None, [16], [0, 256])
        
        features = []
        for hist in [upper_hist_h, upper_hist_s, upper_hist_v, 
                    lower_hist_h, lower_hist_s, lower_hist_v, 
                    edge_hist]:
            normalized = cv2.normalize(hist, hist).flatten()
            features.append(normalized)
        
        combined_features = np.concatenate(features)
        norm = np.linalg.norm(combined_features)
        if norm > 0:
            combined_features = combined_features / norm
        
        return combined_features
    
    def calculate_cosine_similarity(self, feat1, feat2):
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—"""
        if feat1 is None or feat2 is None:
            return None
        
        return np.dot(feat1, feat2) / (np.linalg.norm(feat1) * np.linalg.norm(feat2))

def calculate_iou(box1, box2):
    """IoU (Intersection over Union) ã‚’è¨ˆç®—"""
    x1_1, y1_1, x2_1, y2_1 = box1
    x1_2, y1_2, x2_2, y2_2 = box2
    
    # äº¤å·®é ˜åŸŸ
    x1_i = max(x1_1, x1_2)
    y1_i = max(y1_1, y1_2)
    x2_i = min(x2_1, x2_2)
    y2_i = min(y2_1, y2_2)
    
    if x2_i <= x1_i or y2_i <= y1_i:
        return 0.0
    
    # äº¤å·®é¢ç©
    intersection = (x2_i - x1_i) * (y2_i - y1_i)
    
    # å„ãƒœãƒƒã‚¯ã‚¹ã®é¢ç©
    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
    
    # å’Œé›†åˆé¢ç©
    union = area1 + area2 - intersection
    
    return intersection / union if union > 0 else 0.0

class VideoGalleryTracker:
    """ä¿®æ­£ç‰ˆï¼šå‹•ç”»ç”¨ã‚®ãƒ£ãƒ©ãƒªãƒ¼è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, extractor, config):
        self.extractor = extractor
        self.config = config
        self.galleries = {}  # ID -> {'features': [ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ], 'avg_feature': å¹³å‡ç‰¹å¾´é‡, 'last_seen': ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·, 'bbox': æœ€å¾Œã®bbox}
        self.next_id = 1
        self.frame_count = 0
        self.color_map = {}  # ID -> è‰²ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        self.colors = [(0, 255, 0), (255, 0, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), 
                      (0, 255, 255), (128, 255, 0), (255, 128, 0), (128, 0, 255), (0, 128, 255)]
        self.debug_mode = True  # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
    
    def update_frame(self, frame_num):
        """ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·ã‚’æ›´æ–°"""
        self.frame_count = frame_num
        
        # é•·æœŸé–“è¦‹ãˆã¦ã„ãªã„IDã‚’å‰Šé™¤
        to_remove = []
        for person_id, data in self.galleries.items():
            if frame_num - data['last_seen'] > self.config['max_missing_frames']:
                to_remove.append(person_id)
        
        for person_id in to_remove:
            if self.debug_mode:
                print(f"  ğŸ—‘ï¸ ID {person_id}: {self.config['max_missing_frames']}ãƒ•ãƒ¬ãƒ¼ãƒ è¦‹ãˆãªã„ãŸã‚å‰Šé™¤")
            del self.galleries[person_id]
            if person_id in self.color_map:
                del self.color_map[person_id]
    
    def add_to_gallery(self, person_id, feature, bbox):
        """ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã«ç‰¹å¾´é‡ã‚’è¿½åŠ """
        if person_id not in self.galleries:
            self.galleries[person_id] = {
                'features': [], 
                'avg_feature': None, 
                'last_seen': self.frame_count,
                'bbox': bbox
            }
            # è‰²ã‚’å‰²ã‚Šå½“ã¦
            self.color_map[person_id] = self.colors[len(self.color_map) % len(self.colors)]
        
        self.galleries[person_id]['features'].append(feature)
        self.galleries[person_id]['last_seen'] = self.frame_count
        self.galleries[person_id]['bbox'] = bbox
        
        # ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã‚µã‚¤ã‚ºåˆ¶é™
        if len(self.galleries[person_id]['features']) > self.config['max_gallery_size']:
            self.galleries[person_id]['features'].pop(0)
        
        self._update_average_feature(person_id)
    
    def _update_average_feature(self, person_id):
        """å¹³å‡ç‰¹å¾´é‡ã‚’æ›´æ–°"""
        features = self.galleries[person_id]['features']
        if features:
            avg_feature = np.mean(np.array(features), axis=0)
            avg_feature = avg_feature / np.linalg.norm(avg_feature)
            self.galleries[person_id]['avg_feature'] = avg_feature
    
    def track_detections(self, detections, features):
        """æ¤œå‡ºçµæœã‚’è¿½è·¡ï¼ˆä¿®æ­£ç‰ˆï¼šä¸€å¯¾ä¸€å¯¾å¿œã‚’ä¿è¨¼ï¼‰"""
        if not detections or not features:
            return []
        
        if self.debug_mode and self.frame_count % 30 == 0:
            print(f"  ãƒ•ãƒ¬ãƒ¼ãƒ {self.frame_count}: {len(detections)}å€‹ã®æ¤œå‡ºã‚’å‡¦ç†ä¸­...")
        
        tracking_results = []
        
        if not self.galleries:
            # æ–°è¦IDç™ºè¡Œï¼ˆåˆå›ï¼‰
            for i, (detection, feature) in enumerate(zip(detections, features)):
                new_id = f"ID_{self.next_id:03d}"
                self.next_id += 1
                self.add_to_gallery(new_id, feature, detection['bbox'])
                tracking_results.append({
                    'id': new_id,
                    'bbox': detection['bbox'],
                    'conf': detection['conf'],
                    'similarity': 1.0,
                    'status': 'new'
                })
                if self.debug_mode and self.frame_count % 30 == 0:
                    print(f"    æ–°è¦IDç™ºè¡Œ: {new_id}")
            return tracking_results
        
        # é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ä½œæˆ
        similarity_matrix = []
        gallery_ids = list(self.galleries.keys())
        
        for i, feature in enumerate(features):
            similarities = []
            for person_id in gallery_ids:
                gallery_data = self.galleries[person_id]
                if gallery_data['avg_feature'] is not None:
                    sim = self.extractor.calculate_cosine_similarity(
                        feature, gallery_data['avg_feature']
                    )
                    similarities.append(sim if sim is not None else 0.0)
                else:
                    similarities.append(0.0)
            similarity_matrix.append(similarities)
        
        # ãƒãƒ³ã‚¬ãƒªã‚¢ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆç°¡æ˜“ç‰ˆï¼‰ã«ã‚ˆã‚‹æœ€é©å‰²ã‚Šå½“ã¦
        assignments = self._simple_assignment(similarity_matrix, self.config['gallery_threshold'])
        
        # å‰²ã‚Šå½“ã¦çµæœã‚’å‡¦ç†
        assigned_gallery_indices = set()
        
        for detection_idx, gallery_idx in assignments.items():
            detection = detections[detection_idx]
            feature = features[detection_idx]
            
            if gallery_idx is not None:
                # æ—¢å­˜IDã«å‰²ã‚Šå½“ã¦
                person_id = gallery_ids[gallery_idx]
                similarity = similarity_matrix[detection_idx][gallery_idx]
                
                self.add_to_gallery(person_id, feature, detection['bbox'])
                tracking_results.append({
                    'id': person_id,
                    'bbox': detection['bbox'],
                    'conf': detection['conf'],
                    'similarity': similarity,
                    'status': 'matched'
                })
                assigned_gallery_indices.add(gallery_idx)
                
                if self.debug_mode and self.frame_count % 30 == 0:
                    print(f"    æ¤œå‡º{detection_idx} -> {person_id} (é¡ä¼¼åº¦: {similarity:.3f})")
            else:
                # æ–°è¦IDç™ºè¡Œ
                new_id = f"ID_{self.next_id:03d}"
                self.next_id += 1
                self.add_to_gallery(new_id, feature, detection['bbox'])
                tracking_results.append({
                    'id': new_id,
                    'bbox': detection['bbox'],
                    'conf': detection['conf'],
                    'similarity': 0.0,
                    'status': 'new'
                })
                
                if self.debug_mode and self.frame_count % 30 == 0:
                    print(f"    æ¤œå‡º{detection_idx} -> æ–°è¦ID {new_id}")
        
        return tracking_results
    
    def _simple_assignment(self, similarity_matrix, threshold):
        """ç°¡æ˜“ç‰ˆãƒãƒ³ã‚¬ãƒªã‚¢ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆè²ªæ¬²æ³•ï¼‰"""
        assignments = {}
        used_galleries = set()
        
        # æ¤œå‡ºã¨æ—¢å­˜IDã®ãƒšã‚¢ã‚’é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ
        pairs = []
        for det_idx, similarities in enumerate(similarity_matrix):
            for gal_idx, sim in enumerate(similarities):
                if sim >= threshold:
                    pairs.append((sim, det_idx, gal_idx))
        
        # é¡ä¼¼åº¦é™é †ã§ã‚½ãƒ¼ãƒˆ
        pairs.sort(reverse=True)
        
        # è²ªæ¬²æ³•ã§å‰²ã‚Šå½“ã¦
        for sim, det_idx, gal_idx in pairs:
            if det_idx not in assignments and gal_idx not in used_galleries:
                assignments[det_idx] = gal_idx
                used_galleries.add(gal_idx)
        
        # å‰²ã‚Šå½“ã¦ã‚‰ã‚Œãªã‹ã£ãŸæ¤œå‡ºã¯None
        for det_idx in range(len(similarity_matrix)):
            if det_idx not in assignments:
                assignments[det_idx] = None
        
        return assignments
    
    def get_gallery_stats(self):
        """ã‚®ãƒ£ãƒ©ãƒªãƒ¼çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        stats = {}
        for person_id, data in self.galleries.items():
            stats[person_id] = {
                'gallery_size': len(data['features']),
                'last_seen': data['last_seen'],
                'frames_since_seen': self.frame_count - data['last_seen']
            }
        return stats
    
    def analyze_final_similarities(self):
        """æœ€çµ‚IDé–“é¡ä¼¼åº¦åˆ†æ"""
        if len(self.galleries) < 2:
            print("\nğŸ“Š æœ€çµ‚é¡ä¼¼åº¦åˆ†æ: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–IDãŒ1å€‹ä»¥ä¸‹ã®ãŸã‚åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return
        
        print("\n" + "="*80)
        print("ğŸ“Š æœ€çµ‚IDé–“é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼ˆé™çš„æ¯”è¼ƒï¼‰")
        print("="*80)
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–IDã®ãƒªã‚¹ãƒˆ
        active_ids = list(self.galleries.keys())
        print(f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–IDæ•°: {len(active_ids)}")
        
        # å„IDã®å¹³å‡ç‰¹å¾´é‡ã‚’å–å¾—
        avg_features = {}
        for person_id in active_ids:
            gallery_data = self.galleries[person_id]
            if gallery_data['avg_feature'] is not None:
                avg_features[person_id] = gallery_data['avg_feature']
                print(f"  {person_id}: ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã‚µã‚¤ã‚º{len(gallery_data['features'])}, å¹³å‡ç‰¹å¾´é‡æº–å‚™å®Œäº†")
        
        if len(avg_features) < 2:
            print("\nâš ï¸ å¹³å‡ç‰¹å¾´é‡ãŒåˆ©ç”¨å¯èƒ½ãªIDãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return
        
        # é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ä½œæˆ
        print(f"\nğŸ¤ IDé–“ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹:")
        print("    ", end="")
        for pid in active_ids:
            print(f"{pid:>12}", end="")
        print()
        
        high_similarity_pairs = []
        similarity_matrix = []
        
        for i, id1 in enumerate(active_ids):
            print(f"{id1:>12}", end="")
            row = []
            
            for j, id2 in enumerate(active_ids):
                if i == j:
                    print(f"{'1.000':>12}", end="")
                    row.append(1.0)
                elif id1 in avg_features and id2 in avg_features:
                    feat1 = avg_features[id1]
                    feat2 = avg_features[id2]
                    sim = self.extractor.calculate_cosine_similarity(feat1, feat2)
                    print(f"{sim:>12.3f}", end="")
                    row.append(sim)
                    
                    # é«˜é¡ä¼¼åº¦ãƒšã‚¢ã‚’è¨˜éŒ²ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ i < jï¼‰
                    if sim > 0.7 and i < j:
                        high_similarity_pairs.append((id1, id2, sim))
                else:
                    print(f"{'N/A':>12}", end="")
                    row.append(0.0)
            
            similarity_matrix.append(row)
            print()
        
        # é«˜é¡ä¼¼åº¦ãƒšã‚¢ã®åˆ†æ
        if high_similarity_pairs:
            print(f"\nğŸ¯ é«˜é¡ä¼¼åº¦ãƒšã‚¢åˆ†æï¼ˆé–¾å€¤0.7ä»¥ä¸Šï¼‰:")
            high_similarity_pairs.sort(key=lambda x: x[2], reverse=True)
            
            for id1, id2, sim in high_similarity_pairs:
                print(f"\nã€{id1} â†” {id2}ã€‘")
                print(f"  é¡ä¼¼åº¦: {sim:.4f}")
                
                # ã‚®ãƒ£ãƒ©ãƒªãƒ¼æƒ…å ±
                gallery1 = self.galleries[id1]
                gallery2 = self.galleries[id2]
                print(f"  {id1}: ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã‚µã‚¤ã‚º{len(gallery1['features'])}, æœ€çµ‚ç¢ºèªãƒ•ãƒ¬ãƒ¼ãƒ {gallery1['last_seen']}")
                print(f"  {id2}: ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã‚µã‚¤ã‚º{len(gallery2['features'])}, æœ€çµ‚ç¢ºèªãƒ•ãƒ¬ãƒ¼ãƒ {gallery2['last_seen']}")
                
                if sim > 0.9:
                    print("  ğŸ”¥ æ¥µã‚ã¦é«˜ã„é¡ä¼¼åº¦ â†’ åŒä¸€äººç‰©ã®å¯èƒ½æ€§ãŒéå¸¸ã«é«˜ã„")
                elif sim > 0.85:
                    print("  ğŸš€ éå¸¸ã«é«˜ã„é¡ä¼¼åº¦ â†’ åŒä¸€äººç‰©ã®å¯èƒ½æ€§ãŒé«˜ã„")
                elif sim > 0.8:
                    print("  âœ… é«˜ã„é¡ä¼¼åº¦ â†’ åŒä¸€äººç‰©ã®å¯èƒ½æ€§ã‚ã‚Š")
                else:
                    print("  âš ï¸ ä¸­ç¨‹åº¦ã®é¡ä¼¼åº¦ â†’ è¦æ³¨æ„ãƒšã‚¢")
        else:
            print(f"\nâœ… é«˜é¡ä¼¼åº¦ãƒšã‚¢ï¼ˆ0.7ä»¥ä¸Šï¼‰ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            print("  â†’ å„IDã¯ååˆ†ã«åŒºåˆ¥ã•ã‚Œã¦ã„ã¾ã™")
        
        # åˆ†é›¢æ€§èƒ½è©•ä¾¡
        if len(active_ids) >= 2:
            # æœ€å°IDé–“è·é›¢ã‚’è¨ˆç®—
            min_similarity = 1.0
            min_pair = None
            
            for i, id1 in enumerate(active_ids):
                for j, id2 in enumerate(active_ids):
                    if i < j and id1 in avg_features and id2 in avg_features:
                        sim = similarity_matrix[i][j]
                        if sim < min_similarity:
                            min_similarity = sim
                            min_pair = (id1, id2)
            
            print(f"\nğŸ“ˆ åˆ†é›¢æ€§èƒ½è©•ä¾¡:")
            print(f"  æœ€å°IDé–“é¡ä¼¼åº¦: {min_similarity:.4f} ({min_pair[0]} â†” {min_pair[1]})")
            
            if min_similarity < 0.5:
                print("  ğŸ‰ å„ªç§€ãªåˆ†é›¢æ€§èƒ½ - IDãŒæ˜ç¢ºã«åŒºåˆ¥ã•ã‚Œã¦ã„ã¾ã™")
            elif min_similarity < 0.7:
                print("  âœ… è‰¯å¥½ãªåˆ†é›¢æ€§èƒ½ - å®Ÿç”¨çš„ãªãƒ¬ãƒ™ãƒ«ã§ã™")
            elif min_similarity < 0.8:
                print("  âš ï¸ æ³¨æ„ãŒå¿…è¦ - ä¸€éƒ¨IDã®åŒºåˆ¥ãŒæ›–æ˜§ã§ã™")
            else:
                print("  ğŸš¨ åˆ†é›¢æ€§èƒ½ä¸è¶³ - é–¾å€¤èª¿æ•´ãŒå¿…è¦ã§ã™")
        
        # æ¨å¥¨è¨­å®š
        if high_similarity_pairs:
            max_sim = max([sim for _, _, sim in high_similarity_pairs])
            recommended_threshold = max_sim * 0.95  # æœ€é«˜é¡ä¼¼åº¦ã®95%
            print(f"\nğŸ’¡ æ¨å¥¨ã‚®ãƒ£ãƒ©ãƒªãƒ¼é–¾å€¤: {recommended_threshold:.3f}")
            print(f"  ï¼ˆç¾åœ¨ã®è¨­å®š: {self.config['gallery_threshold']}ï¼‰")

def filter_detections_advanced(detections, image_shape, config):
    """é«˜åº¦ãªæ¤œå‡ºçµæœãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆIoU-NMSä»˜ãï¼‰"""
    h, w = image_shape[:2]
    image_area = h * w
    
    # åŸºæœ¬ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered = []
    for detection in detections:
        x1, y1, x2, y2 = detection['bbox']
        conf = detection['conf']
        
        if conf < config['min_confidence']:
            continue
        
        width = x2 - x1
        height = y2 - y1
        area = width * height
        area_ratio = area / image_area
        
        if (width < config['min_width'] or 
            height < config['min_height'] or 
            area_ratio < config['min_area_ratio']):
            continue
        
        filtered.append(detection)
    
    # IoU-NMSã«ã‚ˆã‚‹é‡è¤‡é™¤å»
    if len(filtered) <= 1:
        return filtered
    
    # ä¿¡é ¼åº¦ã§ã‚½ãƒ¼ãƒˆ
    filtered.sort(key=lambda x: x['conf'], reverse=True)
    
    final_detections = []
    for i, detection in enumerate(filtered):
        keep = True
        for kept_detection in final_detections:
            iou = calculate_iou(detection['bbox'], kept_detection['bbox'])
            if iou > config['iou_threshold']:
                keep = False
                break
        
        if keep:
            final_detections.append(detection)
    
    return final_detections

def process_video_frame(frame, model, tracker, extractor, config):
    """ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    # YOLOv8ã§äººç‰©æ¤œå‡º
    results = model(frame, classes=0, conf=config['min_confidence'], 
                   iou=config['nms_iou_threshold'], verbose=False)
    
    raw_detections = []
    
    if results[0].boxes is not None:
        boxes = results[0].boxes
        
        for box in boxes:
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            conf = box.conf[0].cpu().numpy()
            
            raw_detections.append({
                'bbox': [x1, y1, x2, y2],
                'conf': conf
            })
    
    # é«˜åº¦ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered_detections = filter_detections_advanced(raw_detections, frame.shape, config)
    
    # ç‰¹å¾´é‡æŠ½å‡º
    valid_detections = []
    valid_features = []
    
    for detection in filtered_detections:
        feature = extractor.extract_features(frame, detection['bbox'])
        if feature is not None:
            valid_detections.append(detection)
            valid_features.append(feature)
    
    # è¿½è·¡å®Ÿè¡Œ
    tracking_results = tracker.track_detections(valid_detections, valid_features)
    
    return tracking_results

def draw_tracking_results_debug(frame, tracking_results, tracker, frame_num):
    """è¿½è·¡çµæœã‚’æç”»ï¼ˆãƒ‡ãƒãƒƒã‚°æƒ…å ±ä»˜ãï¼‰"""
    vis_frame = frame.copy()
    
    # é‡è¤‡IDæ¤œè¨¼
    used_ids = set()
    duplicate_ids = set()
    
    for result in tracking_results:
        person_id = result['id']
        if person_id in used_ids:
            duplicate_ids.add(person_id)
        used_ids.add(person_id)
    
    for result in tracking_results:
        person_id = result['id']
        x1, y1, x2, y2 = map(int, result['bbox'])
        conf = result['conf']
        similarity = result['similarity']
        status = result['status']
        
        # è‰²ã‚’å–å¾—ï¼ˆé‡è¤‡IDã¯èµ¤ã§è­¦å‘Šï¼‰
        if person_id in duplicate_ids:
            color = (0, 0, 255)  # èµ¤è‰²ã§è­¦å‘Š
        else:
            color = tracker.color_map.get(person_id, (255, 255, 255))
        
        # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ï¼ˆé‡è¤‡IDã¯å¤ªã„ç·šï¼‰
        thickness = 5 if person_id in duplicate_ids else 3
        cv2.rectangle(vis_frame, (x1, y1), (x2, y2), color, thickness)
        
        # ãƒ©ãƒ™ãƒ«ä½œæˆ
        status_mark = "ğŸ†•" if status == 'new' else "âœ…"
        model_mark = "ğŸ§ " if tracker.extractor.model is not None else "ğŸ“"
        duplicate_mark = "âš ï¸" if person_id in duplicate_ids else ""
        gallery_size = len(tracker.galleries[person_id]['features'])
        
        label = f"{person_id} ({conf:.2f}) {status_mark}{model_mark}{duplicate_mark}"
        sub_label = f"Gallery:{gallery_size} Sim:{similarity:.3f}"
        
        # ãƒ©ãƒ™ãƒ«èƒŒæ™¯
        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
        sub_label_size = cv2.getTextSize(sub_label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
        
        cv2.rectangle(vis_frame, (x1, y1-60), (x1+max(label_size[0], sub_label_size[0])+10, y1), color, -1)
        cv2.putText(vis_frame, label, (x1+5, y1-40), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        cv2.putText(vis_frame, sub_label, (x1+5, y1-20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    # ãƒ•ãƒ¬ãƒ¼ãƒ æƒ…å ±ã¨ã‚¢ãƒ©ãƒ¼ãƒˆè¡¨ç¤º
    info_text = f"Frame: {frame_num} | Active IDs: {len(tracker.galleries)} | Detections: {len(tracking_results)}"
    if duplicate_ids:
        info_text += f" | âš ï¸DUPLICATE: {duplicate_ids}"
    
    cv2.putText(vis_frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    
    return vis_frame

def main():
    print("ğŸ¬ ä¿®æ­£ç‰ˆï¼šå‹•ç”»ã‚®ãƒ£ãƒ©ãƒªãƒ¼è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹ã—ã¾ã™...")
    print(f"è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {CONFIG}")
    
    # å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    video_path = "../videos/megane-test.mov"
        # ../videos/classtest04.mp4: æ•™å®¤ã®å‹•ç”»ã€€é‡ãªã‚Šå¼·
        # ../videos/classtest06-resized.mov: æ•™å®¤ã®å‹•ç”» é‡ãªã‚Šå¼±
        # ../videos/test04.mov: åºƒå ´ã®å‹•ç”» é‡ãªã‚Šä¸­
    output_path = "output_gallery_tracking_fixed.mp4"
    
    if not os.path.exists(video_path):
        print(f"âŒ å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {video_path}")
        return
    
    # YOLOv8ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    model = YOLO('yolov8m.pt')
    
    # OSNetç‰¹å¾´é‡æŠ½å‡ºå™¨ã‚’åˆæœŸåŒ–
    extractor = OSNetFeatureExtractor(CONFIG['reid_model'])
    
    # ã‚®ãƒ£ãƒ©ãƒªãƒ¼è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
    tracker = VideoGalleryTracker(extractor, CONFIG)
    
    # å‹•ç”»èª­ã¿è¾¼ã¿
    cap = cv2.VideoCapture(video_path)
    
    if not cap.isOpened():
        print(f"âŒ å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã‘ã¾ã›ã‚“: {video_path}")
        return
    
    # å‹•ç”»æƒ…å ±å–å¾—
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"ğŸ“¹ å‹•ç”»æƒ…å ±: {width}x{height}, {fps}FPS, {total_frames}ãƒ•ãƒ¬ãƒ¼ãƒ ")
    
    # å‡ºåŠ›å‹•ç”»è¨­å®š
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, CONFIG['video_fps'], (width, height))
    
    frame_num = 0
    process_start_time = time.time()
    
    print("ğŸš€ ä¿®æ­£ç‰ˆå‹•ç”»å‡¦ç†ã‚’é–‹å§‹...")
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_num += 1
            tracker.update_frame(frame_num)
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†
            tracking_results = process_video_frame(frame, model, tracker, extractor, CONFIG)
            
            # çµæœæç”»ï¼ˆãƒ‡ãƒãƒƒã‚°ä»˜ãï¼‰
            vis_frame = draw_tracking_results_debug(frame, tracking_results, tracker, frame_num)
            
            # å‡ºåŠ›å‹•ç”»ã«æ›¸ãè¾¼ã¿
            out.write(vis_frame)
            
            # é€²æ—è¡¨ç¤º
            if frame_num % 30 == 0:
                elapsed_time = time.time() - process_start_time
                fps_current = frame_num / elapsed_time
                progress = (frame_num / total_frames) * 100
                
                print(f"  ğŸ“Š ãƒ•ãƒ¬ãƒ¼ãƒ  {frame_num}/{total_frames} ({progress:.1f}%) | "
                      f"å‡¦ç†é€Ÿåº¦: {fps_current:.1f}FPS | ã‚¢ã‚¯ãƒ†ã‚£ãƒ–IDæ•°: {len(tracker.galleries)}")
                
                # ã‚®ãƒ£ãƒ©ãƒªãƒ¼çµ±è¨ˆè¡¨ç¤º
                stats = tracker.get_gallery_stats()
                for person_id, stat in stats.items():
                    print(f"    {person_id}: ã‚®ãƒ£ãƒ©ãƒªãƒ¼{stat['gallery_size']}å€‹, "
                          f"æœ€çµ‚ç¢ºèªã‹ã‚‰{stat['frames_since_seen']}ãƒ•ãƒ¬ãƒ¼ãƒ ")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚Šä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    
    finally:
        # ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾
        cap.release()
        out.release()
        cv2.destroyAllWindows()
        
        total_time = time.time() - process_start_time
        avg_fps = frame_num / total_time
        
        print(f"\nğŸ¬ ä¿®æ­£ç‰ˆå‹•ç”»å‡¦ç†å®Œäº†!")
        print(f"ğŸ“Š å‡¦ç†çµ±è¨ˆ:")
        print(f"  ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {frame_num}")
        print(f"  ç·å‡¦ç†æ™‚é–“: {total_time:.1f}ç§’")
        print(f"  å¹³å‡å‡¦ç†é€Ÿåº¦: {avg_fps:.1f}FPS")
        print(f"  ç™ºè¡ŒIDæ•°: {tracker.next_id - 1}")
        print(f"  æœ€çµ‚ã‚¢ã‚¯ãƒ†ã‚£ãƒ–IDæ•°: {len(tracker.galleries)}")
        print(f"  å‡ºåŠ›å‹•ç”»: {output_path}")
        
        # æœ€çµ‚ã‚®ãƒ£ãƒ©ãƒªãƒ¼çµ±è¨ˆ
        print(f"\nğŸ›ï¸ æœ€çµ‚ã‚®ãƒ£ãƒ©ãƒªãƒ¼çµ±è¨ˆ:")
        stats = tracker.get_gallery_stats()
        for person_id, stat in stats.items():
            print(f"  {person_id}: ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã‚µã‚¤ã‚º{stat['gallery_size']}, "
                  f"æœ€çµ‚ç¢ºèªãƒ•ãƒ¬ãƒ¼ãƒ {stat['last_seen']}")
        
        # æœ€çµ‚IDé–“é¡ä¼¼åº¦åˆ†æ
        tracker.analyze_final_similarities()

if __name__ == "__main__":
    main() 